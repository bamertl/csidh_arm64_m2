/* DO NOT EDIT! generated by autogen */

#include "uintbig_namespace.h"
#include "fp_namespace.h"
.extern uintbig_p
.extern uintbig_mul3_64
.extern uintbig_mul3_64_full
.extern uintbig_add3
.extern uintbig_sub3


.set pbits, 1020 
.set pbytes, 128 
.set plimbs, 16 

.global fp_0
fp_0: 
	.zero 64
	/* 2^1024 mod p */
.global fp_1
fp_1: 
    .quad 0x65e7ee6590e6567d, 0x40a5f2587fef86d4, 0x99f9e607b99d62f2, 0x1089df50f4f8f26d
    .quad 0x592890dd02bb585a, 0xe1b6be68b969ecb9, 0xaebe3c10395f33c3, 0x5ef9652396531f1b
    .quad 0x28d37db76b7a1b7f, 0x86d089fa474b4a3f, 0xdbce120cc7a4fff2, 0x08b3f947137340ac
    .quad 0x913f3e7c71b37ce5, 0xc7d1b17b09ec4577, 0x9d834aff6f7956b6, 0x044c4b3e968ec2b8

	/* 2^1025 mod p */
.global fp_2
fp_2: 
    .quad 0xcbcfdccb21ccacfa, 0x814be4b0ffdf0da8, 0x33f3cc0f733ac5e4, 0x2113bea1e9f1e4db
    .quad 0xb25121ba0576b0b4, 0xc36d7cd172d3d972, 0x5d7c782072be6787, 0xbdf2ca472ca63e37
    .quad 0x51a6fb6ed6f436fe, 0x0da113f48e96947e, 0xb79c24198f49ffe5, 0x1167f28e26e68159
    .quad 0x227e7cf8e366f9ca, 0x8fa362f613d88aef, 0x3b0695fedef2ad6d, 0x0898967d2d1d8571

	/* (2^1024)^2 mod p */
.r_squared_mod_p: /* (2^1024)^2 mod p */
    .quad 0xd6b8f146ec5055af, 0x68ac5d7707ccb03a, 0x1322c9b9837dca17, 0x4f2940830c1d2b35
    .quad 0x8c1a56e5bf96471a, 0x6cdde00636c4f801, 0x9365ec4fa327c9ac, 0xa0056a67c1de0e82
    .quad 0x8aa6fa7e6811faa8, 0x9aad9631bb760403, 0x156b34c683839b9d, 0xa5ae047480992b2c
    .quad 0xc124d930289048b5, 0x4f8a8344bbe56288, 0xe1a2eb1d838b8237, 0x057162f911ca93a3

.data
.global fp_mulsq_count
fp_mulsq_count: 
	.quad 0
.global fp_sq_count
fp_sq_count: 
	.quad 0
.global fp_addsub_count
fp_addsub_count: 
	.quad 0

.text
.align 4

	/* [x1] = [x0]  x0 is being copied to x1 */
.global fp_copy
fp_copy: 
	ldp x3, x4, [x0, #0]
	ldp x5, x6, [x0, #16]
	ldp x7, x8, [x0, #32]
	ldp x9, x10, [x0, #48]
	stp x3, x4, [x1, #0]
	stp x5, x6, [x1, #16]
	stp x7, x8, [x1, #32]
	stp x9, x10, [x1, #48]
	ldp x3, x4, [x0, #64]
	ldp x5, x6, [x0, #80]
	ldp x7, x8, [x0, #96]
	ldp x9, x10, [x0, #112]
	stp x3, x4, [x1, #64]
	stp x5, x6, [x1, #80]
	stp x7, x8, [x1, #96]
	stp x9, x10, [x1, #112]
	ret

	/* A[x0] = B[x1] if d[x2], we make it time-constant */
.global fp_cmov
fp_cmov: 
	/* (AxorB)^d xor A = A */
	/* A xor 0 = A */
	/*  d-1 = 1111 or 0000 that why it is like this */
	/* A xor B and 1 xor B = A */
	/* A xor B and 0 xor B = B */
	subs x15, x2, #1
	/* Batch 0 */
	ldp x3, x4, [x0, #0]
	ldp x7, x8, [x1, #0]
	ldp x5, x6, [x0, #16]
	ldp x9, x10, [x1, #16]
	eor x11, x3, x7 // A xor B 
	eor x12, x4, x8
	eor x13, x5, x9
	eor x14, x6, x10

	and x11, x11, x15 // A xor B and d 
	and x12, x12, x15
	and x13, x13, x15
	and x14, x14, x15

	eor x11, x11, x7 // A xor B and d xor B 
	eor x12, x12, x8
	eor x13, x13, x9
	eor x14, x14, x10

	stp x11, x12, [x0, #0]
	stp x13, x14, [x0, #16]
	/* Batch 1 */
	ldp x3, x4, [x0, #32]
	ldp x7, x8, [x1, #32]
	ldp x5, x6, [x0, #48]
	ldp x9, x10, [x1, #48]
	eor x11, x3, x7 // A xor B 
	eor x12, x4, x8
	eor x13, x5, x9
	eor x14, x6, x10

	and x11, x11, x15 // A xor B and d 
	and x12, x12, x15
	and x13, x13, x15
	and x14, x14, x15

	eor x11, x11, x7 // A xor B and d xor B 
	eor x12, x12, x8
	eor x13, x13, x9
	eor x14, x14, x10

	stp x11, x12, [x0, #32]
	stp x13, x14, [x0, #48]
	/* Batch 2 */
	ldp x3, x4, [x0, #64]
	ldp x7, x8, [x1, #64]
	ldp x5, x6, [x0, #80]
	ldp x9, x10, [x1, #80]
	eor x11, x3, x7 // A xor B 
	eor x12, x4, x8
	eor x13, x5, x9
	eor x14, x6, x10

	and x11, x11, x15 // A xor B and d 
	and x12, x12, x15
	and x13, x13, x15
	and x14, x14, x15

	eor x11, x11, x7 // A xor B and d xor B 
	eor x12, x12, x8
	eor x13, x13, x9
	eor x14, x14, x10

	stp x11, x12, [x0, #64]
	stp x13, x14, [x0, #80]
	/* Batch 3 */
	ldp x3, x4, [x0, #96]
	ldp x7, x8, [x1, #96]
	ldp x5, x6, [x0, #112]
	ldp x9, x10, [x1, #112]
	eor x11, x3, x7 // A xor B 
	eor x12, x4, x8
	eor x13, x5, x9
	eor x14, x6, x10

	and x11, x11, x15 // A xor B and d 
	and x12, x12, x15
	and x13, x13, x15
	and x14, x14, x15

	eor x11, x11, x7 // A xor B and d xor B 
	eor x12, x12, x8
	eor x13, x13, x9
	eor x14, x14, x10

	stp x11, x12, [x0, #96]
	stp x13, x14, [x0, #112]
	ret 

	/* A[x0] <-> B[x1] if d[x2], we make it time-constant */
.global fp_cswap
fp_cswap: 
	subs x15, x2, #1
	/* Batch 0 */
	ldp x3, x4, [x0, #0]
	ldp x7, x8, [x1, #0]
	ldp x5, x6, [x0, #16]
	ldp x9, x10, [x1, #16]

	eor x11, x3, x7 // A xor B 
	eor x12, x4, x8
	eor x13, x5, x9
	eor x14, x6, x10

	and x11, x11, x15 // A xor B and d 
	and x12, x12, x15
	and x13, x13, x15
	and x14, x14, x15

	eor x7, x11, x7 // A xor B and d xor B for A 
	eor x8, x12, x8
	eor x9, x13, x9
	eor x10, x14, x10
	stp x7, x8, [x0, #0]
	stp x9, x10, [x0, #16]

	eor x3, x11, x3 // A xor B and d xor A for B 
	eor x4, x12, x4
	eor x5, x13, x5
	eor x6, x14, x6
	stp x3, x4, [x1, #0]
	stp x5, x6, [x1, #16]
	/* Batch 1 */
	ldp x3, x4, [x0, #32]
	ldp x7, x8, [x1, #32]
	ldp x5, x6, [x0, #48]
	ldp x9, x10, [x1, #48]

	eor x11, x3, x7 // A xor B 
	eor x12, x4, x8
	eor x13, x5, x9
	eor x14, x6, x10

	and x11, x11, x15 // A xor B and d 
	and x12, x12, x15
	and x13, x13, x15
	and x14, x14, x15

	eor x7, x11, x7 // A xor B and d xor B for A 
	eor x8, x12, x8
	eor x9, x13, x9
	eor x10, x14, x10
	stp x7, x8, [x0, #32]
	stp x9, x10, [x0, #48]

	eor x3, x11, x3 // A xor B and d xor A for B 
	eor x4, x12, x4
	eor x5, x13, x5
	eor x6, x14, x6
	stp x3, x4, [x1, #32]
	stp x5, x6, [x1, #48]
	/* Batch 2 */
	ldp x3, x4, [x0, #64]
	ldp x7, x8, [x1, #64]
	ldp x5, x6, [x0, #80]
	ldp x9, x10, [x1, #80]

	eor x11, x3, x7 // A xor B 
	eor x12, x4, x8
	eor x13, x5, x9
	eor x14, x6, x10

	and x11, x11, x15 // A xor B and d 
	and x12, x12, x15
	and x13, x13, x15
	and x14, x14, x15

	eor x7, x11, x7 // A xor B and d xor B for A 
	eor x8, x12, x8
	eor x9, x13, x9
	eor x10, x14, x10
	stp x7, x8, [x0, #64]
	stp x9, x10, [x0, #80]

	eor x3, x11, x3 // A xor B and d xor A for B 
	eor x4, x12, x4
	eor x5, x13, x5
	eor x6, x14, x6
	stp x3, x4, [x1, #64]
	stp x5, x6, [x1, #80]
	/* Batch 3 */
	ldp x3, x4, [x0, #96]
	ldp x7, x8, [x1, #96]
	ldp x5, x6, [x0, #112]
	ldp x9, x10, [x1, #112]

	eor x11, x3, x7 // A xor B 
	eor x12, x4, x8
	eor x13, x5, x9
	eor x14, x6, x10

	and x11, x11, x15 // A xor B and d 
	and x12, x12, x15
	and x13, x13, x15
	and x14, x14, x15

	eor x7, x11, x7 // A xor B and d xor B for A 
	eor x8, x12, x8
	eor x9, x13, x9
	eor x10, x14, x10
	stp x7, x8, [x0, #96]
	stp x9, x10, [x0, #112]

	eor x3, x11, x3 // A xor B and d xor A for B 
	eor x4, x12, x4
	eor x5, x13, x5
	eor x6, x14, x6
	stp x3, x4, [x1, #96]
	stp x5, x6, [x1, #112]
	ret 

	/* A[x0] = A[x0] + B[x1] */
.global fp_add2
fp_add2: 
	/* A[x0] = B[x1] + C[x2] */
.global fp_add3
fp_add3: 
	/* Increment add/sub counter */
	adrp x3, fp_addsub_count @PAGE
	add x3, x3, fp_addsub_count@PAGEOFF
	ldr x4, [x3]
	add x4, x4, #1
	str x4, [x3]

	sub sp, sp, #32
	stp x21, x22, [sp, #0]
	stp x23, lr, [sp, #16]
	mov x21, x0  
	adrp x23, uintbig_p @PAGE
	add x23, x23, uintbig_p@PAGEOFF
	bl uintbig_add3 // x0 is now the carry 
	mov x22, x0  
	ldp x3, x4, [x21, #0]  
	ldp x7, x8, [x23, #0]  
	ldp x5, x6, [x21, #16]  
	ldp x9, x10, [x23, #16]  
	subs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x21, #0]  
	stp x13, x14, [x21, #16]  
	ldp x3, x4, [x21, #32]  
	ldp x7, x8, [x23, #32]  
	ldp x5, x6, [x21, #48]  
	ldp x9, x10, [x23, #48]  
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x21, #32]  
	stp x13, x14, [x21, #48]  
	ldp x3, x4, [x21, #64]  
	ldp x7, x8, [x23, #64]  
	ldp x5, x6, [x21, #80]  
	ldp x9, x10, [x23, #80]  
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x21, #64]  
	stp x13, x14, [x21, #80]  
	ldp x3, x4, [x21, #96]  
	ldp x7, x8, [x23, #96]  
	ldp x5, x6, [x21, #112]  
	ldp x9, x10, [x23, #112]  
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x21, #96]  
	stp x13, x14, [x21, #112]  
	sbcs x22, x22, xzr  
	sbcs x22, xzr, xzr  
	/* carry again into x22 */
	ldp x3, x4, [x21, #0]  
	ldp x7, x8, [x23, #0]  
	ldp x5, x6, [x21, #16]  
	ldp x9, x10, [x23, #16]  
	and x7, x7, x22  
	and x8, x8, x22  
	and x9, x9, x22  
	and x10, x10, x22  
	adds x11, x3, x7  
	adcs x12, x4, x8  
	adcs x13, x5, x9  
	adcs x14, x6, x10  
	stp x11, x12, [x21, #0]  
	stp x13, x14, [x21, #16]  
	ldp x3, x4, [x21, #32]  
	ldp x7, x8, [x23, #32]  
	ldp x5, x6, [x21, #48]  
	ldp x9, x10, [x23, #48]  
	and x7, x7, x22  
	and x8, x8, x22  
	and x9, x9, x22  
	and x10, x10, x22  
	adcs x11, x3, x7  
	adcs x12, x4, x8  
	adcs x13, x5, x9  
	adcs x14, x6, x10  
	stp x11, x12, [x21, #32]  
	stp x13, x14, [x21, #48]  
	ldp x3, x4, [x21, #64]  
	ldp x7, x8, [x23, #64]  
	ldp x5, x6, [x21, #80]  
	ldp x9, x10, [x23, #80]  
	and x7, x7, x22  
	and x8, x8, x22  
	and x9, x9, x22  
	and x10, x10, x22  
	adcs x11, x3, x7  
	adcs x12, x4, x8  
	adcs x13, x5, x9  
	adcs x14, x6, x10  
	stp x11, x12, [x21, #64]  
	stp x13, x14, [x21, #80]  
	ldp x3, x4, [x21, #96]  
	ldp x7, x8, [x23, #96]  
	ldp x5, x6, [x21, #112]  
	ldp x9, x10, [x23, #112]  
	and x7, x7, x22  
	and x8, x8, x22  
	and x9, x9, x22  
	and x10, x10, x22  
	adcs x11, x3, x7  
	adcs x12, x4, x8  
	adcs x13, x5, x9  
	adcs x14, x6, x10  
	stp x11, x12, [x21, #96]  
	stp x13, x14, [x21, #112]  
	ldp x21, x22, [sp, #0]
	ldp x23, lr, [sp, #16]
	add sp, sp, #32
	ret

	/* A[x0] = A[x0] - B[x1] */
.global fp_sub2
fp_sub2: 
	mov x2, x1
	mov x1, x0
	/* A[x0] = A[x0] - B[x1] - C[x2] */
.global fp_sub3
fp_sub3: 
	sub sp, sp, #160
	stp lr, x19, [sp, #0]
	stp x20, x21, [sp, #16]
	/* Increment add/sub counter */
	adrp x3, fp_addsub_count @PAGE
	add x3, x3, fp_addsub_count@PAGEOFF
	ldr x4, [x3]
	add x4, x4, #1
	str x4, [x3]

	mov x19, x0
	mov x20, x1
	/* stack + 32 = -x2 */
	mov x1, x2
	add x0, sp, #32
	bl _minus_number
	mov x2, x0
	mov x0, x19
	mov x1, x20
	bl fp_add3
	ldp lr, x19, [sp, #0]
	ldp x20, x21, [sp, #16]
	add sp, sp, #160
	ret

	/* x0 = -x1 */
_minus_number: 
	sub sp, sp, #32
	stp x19, x20, [sp, #0]
	str x21, [sp, #16]
	adrp x20, uintbig_p @PAGE
	add x20, x20, uintbig_p@PAGEOFF
	mov x21, xzr  
	/* P-A */
	ldp x3, x4, [x1, #0]  
	ldp x7, x8, [x20, #0]  
	ldp x5, x6, [x1, #16]  
	ldp x9, x10, [x20, #16]  
	subs x11, x7, x3  
	orr x21, x21, x11  
	sbcs x12, x8, x4  
	orr x21, x21, x12  
	sbcs x13, x9, x5  
	orr x21, x21, x13  
	sbcs x14, x10, x6  
	orr x21, x21, x14  
	stp x11, x12, [x0, #0]  
	stp x13, x14, [x0, #16]  
	ldp x3, x4, [x1, #32]  
	ldp x7, x8, [x20, #32]  
	ldp x5, x6, [x1, #48]  
	ldp x9, x10, [x20, #48]  
	sbcs x11, x7, x3  
	orr x21, x21, x11  
	sbcs x12, x8, x4  
	orr x21, x21, x12  
	sbcs x13, x9, x5  
	orr x21, x21, x13  
	sbcs x14, x10, x6  
	orr x21, x21, x14  
	stp x11, x12, [x0, #32]  
	stp x13, x14, [x0, #48]  
	ldp x3, x4, [x1, #64]  
	ldp x7, x8, [x20, #64]  
	ldp x5, x6, [x1, #80]  
	ldp x9, x10, [x20, #80]  
	sbcs x11, x7, x3  
	orr x21, x21, x11  
	sbcs x12, x8, x4  
	orr x21, x21, x12  
	sbcs x13, x9, x5  
	orr x21, x21, x13  
	sbcs x14, x10, x6  
	orr x21, x21, x14  
	stp x11, x12, [x0, #64]  
	stp x13, x14, [x0, #80]  
	ldp x3, x4, [x1, #96]  
	ldp x7, x8, [x20, #96]  
	ldp x5, x6, [x1, #112]  
	ldp x9, x10, [x20, #112]  
	sbcs x11, x7, x3  
	orr x21, x21, x11  
	sbcs x12, x8, x4  
	orr x21, x21, x12  
	sbcs x13, x9, x5  
	orr x21, x21, x13  
	sbcs x14, x10, x6  
	orr x21, x21, x14  
	stp x11, x12, [x0, #96]  
	stp x13, x14, [x0, #112]  
	cmp x21, #0
	cset x21, eq
	lsl x20, x20, #63
	asr x20, x20, #63
	/* and the prime (if a was 0 then we and with 1, otherwise 0) */
	ldp x3, x4, [x0, #0]  
	ldp x7, x8, [x20, #0]  
	ldp x5, x6, [x0, #16]  
	ldp x9, x10, [x20, #16]  
	and x7, x7, x21  
	and x8, x8, x21  
	and x9, x9, x21  
	and x10, x10, x21  
	subs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x0, #0]  
	stp x13, x14, [x0, #16]  
	ldp x3, x4, [x0, #32]  
	ldp x7, x8, [x20, #32]  
	ldp x5, x6, [x0, #48]  
	ldp x9, x10, [x20, #48]  
	and x7, x7, x21  
	and x8, x8, x21  
	and x9, x9, x21  
	and x10, x10, x21  
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x0, #32]  
	stp x13, x14, [x0, #48]  
	ldp x3, x4, [x0, #64]  
	ldp x7, x8, [x20, #64]  
	ldp x5, x6, [x0, #80]  
	ldp x9, x10, [x20, #80]  
	and x7, x7, x21  
	and x8, x8, x21  
	and x9, x9, x21  
	and x10, x10, x21  
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x0, #64]  
	stp x13, x14, [x0, #80]  
	ldp x3, x4, [x0, #96]  
	ldp x7, x8, [x20, #96]  
	ldp x5, x6, [x0, #112]  
	ldp x9, x10, [x20, #112]  
	and x7, x7, x21  
	and x8, x8, x21  
	and x9, x9, x21  
	and x10, x10, x21  
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x0, #96]  
	stp x13, x14, [x0, #112]  
	ldp x19, x20, [sp, #0]
	ldr x21, [sp, #16]
	add sp, sp, #32
	ret

	/* x0 = x0^2, straight to fp_sq2 */
.global fp_sq1
fp_sq1: 
	mov x1, x0
	/* x0 = x1^2 mod p */
.global fp_sq2
fp_sq2: 
	/* Increment sq counter */
	adrp x3, fp_sq_count @PAGE
	add x3, x3, fp_sq_count@PAGEOFF
	ldr x4, [x3]
	add x4, x4, #1
	str x4, [x3]

	mov x2, x1
	b fp_mul3
