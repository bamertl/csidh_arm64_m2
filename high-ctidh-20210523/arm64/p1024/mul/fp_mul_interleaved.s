/* DO NOT EDIT! generated by autogen */

#include "../../../uintbig_namespace.h"
#include "../../../fp_namespace.h"
.extern uintbig_p
.extern fp_mulsq_count

inv_min_p_mod_r: 
	.quad 0xd2c2c24160038025 
.text
.align 4 

/*
 [C_ADR]+1 = [C_ADR] + [B_ADR] * AI   
*/ 
.macro MUL_LIMBSx1, AI, B_ADR, C_ADR, CARRY_REG, C0, C1, C2, C3, T0, T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14, T15 
	adds \CARRY_REG, xzr, xzr  // CARRY_REG = 0 

	/* LIMBS C0-C3 */
	ldp \T0, \T1, [\B_ADR, #0]  // Load B 
	ldp \C0, \C1, [\C_ADR, #0]  // Load C 
	ldp \T2, \T3, [\B_ADR, #16]  // Load B 
	ldp \C2, \C3, [\C_ADR, #16]  // Load C 

	mul \T8, \T0, \AI  
	umulh \T9, \T0, \AI  
	mul \T10, \T1, \AI  
	umulh \T11, \T1, \AI  
	mul \T12, \T2, \AI  
	umulh \T13, \T2, \AI  
	mul \T14, \T3, \AI  
	umulh \T15, \T3, \AI  

	adcs \C0, \C0, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C0, \C0, \T8  // add C0 

	adcs \C1, \C1, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C1, \C1, \T9  // add C1 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C1, \C1, \T10  // add T 

	adcs \C2, \C2, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C2, \C2, \T11  // add C2 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C2, \C2, \T12  // add T 

	adcs \C3, \C3, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C3, \C3, \T13  // add C3 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C3, \C3, \T14  // add T 

	adcs \CARRY_REG, \CARRY_REG, \T15  
	stp \C0, \C1, [\C_ADR, #0]  // Store C 
	stp \C2, \C3, [\C_ADR, #16]  

	/* LIMBS C4-C7 */
	ldp \T0, \T1, [\B_ADR, #32]  // Load B 
	ldp \C0, \C1, [\C_ADR, #32]  // Load C 
	ldp \T2, \T3, [\B_ADR, #48]  // Load B 
	ldp \C2, \C3, [\C_ADR, #48]  // Load C 

	mul \T8, \T0, \AI  
	umulh \T9, \T0, \AI  
	mul \T10, \T1, \AI  
	umulh \T11, \T1, \AI  
	mul \T12, \T2, \AI  
	umulh \T13, \T2, \AI  
	mul \T14, \T3, \AI  
	umulh \T15, \T3, \AI  

	adcs \C0, \C0, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C0, \C0, \T8  // add C4 

	adcs \C1, \C1, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C1, \C1, \T9  // add C5 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C1, \C1, \T10  // add T 

	adcs \C2, \C2, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C2, \C2, \T11  // add C6 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C2, \C2, \T12  // add T 

	adcs \C3, \C3, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C3, \C3, \T13  // add C7 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C3, \C3, \T14  // add T 

	adcs \CARRY_REG, \CARRY_REG, \T15  
	stp \C0, \C1, [\C_ADR, #32]  // Store C 
	stp \C2, \C3, [\C_ADR, #48]  

	/* LIMBS C8-C11 */
	ldp \T0, \T1, [\B_ADR, #64]  // Load B 
	ldp \C0, \C1, [\C_ADR, #64]  // Load C 
	ldp \T2, \T3, [\B_ADR, #80]  // Load B 
	ldp \C2, \C3, [\C_ADR, #80]  // Load C 

	mul \T8, \T0, \AI  
	umulh \T9, \T0, \AI  
	mul \T10, \T1, \AI  
	umulh \T11, \T1, \AI  
	mul \T12, \T2, \AI  
	umulh \T13, \T2, \AI  
	mul \T14, \T3, \AI  
	umulh \T15, \T3, \AI  

	adcs \C0, \C0, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C0, \C0, \T8  // add C8 

	adcs \C1, \C1, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C1, \C1, \T9  // add C9 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C1, \C1, \T10  // add T 

	adcs \C2, \C2, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C2, \C2, \T11  // add C10 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C2, \C2, \T12  // add T 

	adcs \C3, \C3, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C3, \C3, \T13  // add C11 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C3, \C3, \T14  // add T 

	adcs \CARRY_REG, \CARRY_REG, \T15  
	stp \C0, \C1, [\C_ADR, #64]  // Store C 
	stp \C2, \C3, [\C_ADR, #80]  

	/* LIMBS C12-C15 */
	ldp \T0, \T1, [\B_ADR, #96]  // Load B 
	ldp \C0, \C1, [\C_ADR, #96]  // Load C 
	ldp \T2, \T3, [\B_ADR, #112]  // Load B 
	ldp \C2, \C3, [\C_ADR, #112]  // Load C 

	mul \T8, \T0, \AI  
	umulh \T9, \T0, \AI  
	mul \T10, \T1, \AI  
	umulh \T11, \T1, \AI  
	mul \T12, \T2, \AI  
	umulh \T13, \T2, \AI  
	mul \T14, \T3, \AI  
	umulh \T15, \T3, \AI  

	adcs \C0, \C0, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C0, \C0, \T8  // add C12 

	adcs \C1, \C1, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C1, \C1, \T9  // add C13 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C1, \C1, \T10  // add T 

	adcs \C2, \C2, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C2, \C2, \T11  // add C14 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C2, \C2, \T12  // add T 

	adcs \C3, \C3, \CARRY_REG  // add carry 
	adcs \CARRY_REG, xzr, xzr  
	adcs \C3, \C3, \T13  // add C15 
	adcs \CARRY_REG, \CARRY_REG, xzr  
	adcs \C3, \C3, \T14  // add T 

	adcs \CARRY_REG, \CARRY_REG, \T15  
	stp \C0, \C1, [\C_ADR, #96]  // Store C 
	stp \C2, \C3, [\C_ADR, #112]  
	/* Store last C at [C_ADR]+1, which means offset: 128 */
	ldr \C0, [\C_ADR, #128]  
	adcs \C0, \C0, \CARRY_REG  // add carry 
	str \C0, [\C_ADR, #128] 

.endm 

/*
 C ← C + ai B 
 q ← μC mod r
 C ← (C + Nq)/r  
*/ 
.macro MUL_STEP, K, A_ADR, B_ADR, C_ADR, P_ADR, AI, CARRY_REG, C0, C1, C2, C3, T0, T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14, T15 
	ldr \AI, [\A_ADR ,\K] // load AI 
	/* C ← C + ai B */
	MUL_LIMBSx1 \AI, \B_ADR, \C_ADR, \CARRY_REG, \C0, \C1, \C2, \C3, \T0, \T1, \T2, \T3, \T4, \T5, \T6, \T7, \T8, \T9, \T10, \T11, \T12, \T13, \T14, \T15 
	/* q ← μC mod r , we just need to multiply C[0] with inv_min_p_mod_r */
	adrp \T0, inv_min_p_mod_r@PAGE
	add \T0, \T0, inv_min_p_mod_r@PAGEOFF
	ldr \T0, [\T0] // load inv_min_p_mod_r 
	ldr \T1, [\C_ADR] // load C[0] 
	mul \AI, \T0, \T1  // q ← μC mod r 
	/* C ← (C + Nq)/r */
	MUL_LIMBSx1 \AI, \P_ADR, \C_ADR, \CARRY_REG, \C0, \C1, \C2, \C3, \T0, \T1, \T2, \T3, \T4, \T5, \T6, \T7, \T8, \T9, \T10, \T11, \T12, \T13, \T14, \T15 
	/* We shift C  */
	ldr \C0, [\C_ADR, #128]  
	ldp \T0, \T1, [\C_ADR, #0]  
	ldp \T2, \T3, [\C_ADR, #16]  
	str \T1, [\C_ADR, #0]  
	stp \T2, \T3, [\C_ADR, #8]  
	ldp \T0, \T1, [\C_ADR, #32]  
	ldp \T2, \T3, [\C_ADR, #48]  
	stp \T0, \T1, [\C_ADR, #24]  
	stp \T2, \T3, [\C_ADR, #40]  
	ldp \T0, \T1, [\C_ADR, #64]  
	ldp \T2, \T3, [\C_ADR, #80]  
	stp \T0, \T1, [\C_ADR, #56]  
	stp \T2, \T3, [\C_ADR, #72]  
	ldp \T0, \T1, [\C_ADR, #96]  
	ldp \T2, \T3, [\C_ADR, #112]  
	stp \T0, \T1, [\C_ADR, #88]  
	stp \T2, \T3, [\C_ADR, #104]  
	stp \C0, xzr, [\C_ADR, #120]  

.endm 

	/* A[x0] = A[x0] * B[x1] mod P 
 directly into fp_mul3 */
.global fp_mul2
fp_mul2: 
	mov x2, x0
	/* Montgomery multiplication
 C[x0] = A[x1] * B[x2] mod P */
.global fp_mul3
fp_mul3: 
	/* Increment mul/sq counter */
	adrp x3, fp_mulsq_count@PAGE
	add x3, x3, fp_mulsq_count@PAGEOFF
	ldr x4, [x3]
	add x4, x4, #1
	str x4, [x3]

	sub sp, sp, #240
	stp lr, x0, [sp, #0]
	stp x19, x20, [sp, #16]
	stp x21, x22, [sp, #32]
	stp x23, x24, [sp, #48]
	stp x25, x26, [sp, #64]
	stp x27, x28, [sp, #80]
	str x30, [sp, #96]

	/* Load adress of P, A and B and C into respective registers */
	adrp x6, uintbig_p@PAGE
	add x6, x6, uintbig_p@PAGEOFF
	add x3, sp, #104  
	mov x1, x1  
	mov x2, x2  
	/* Init C to 0 */
	stp xzr, xzr, [x3, #0]  
	stp xzr, xzr, [x3, #16]  
	stp xzr, xzr, [x3, #32]  
	stp xzr, xzr, [x3, #48]  
	stp xzr, xzr, [x3, #64]  
	stp xzr, xzr, [x3, #80]  
	stp xzr, xzr, [x3, #96]  
	stp xzr, xzr, [x3, #112]  
	str xzr, [x3, #128] // set C[17] to 0 

	MUL_STEP #0, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #8, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #16, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #24, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #32, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #40, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #48, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #56, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #64, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #72, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #80, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #88, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #96, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #104, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #112, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 
	MUL_STEP #120, x1, x2, x3, x6, x4, x11, x7, x8, x9, x10, x12, x13, x14, x15, x16, x17, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28 

	/* Store Result C to [x0] and the overflow into x1 */
	ldp x12, x13, [x3, #0]  
	ldp x14, x15, [x3, #16]  
	stp x12, x13, [x0, #0]  
	stp x14, x15, [x0, #16]  
	ldp x12, x13, [x3, #32]  
	ldp x14, x15, [x3, #48]  
	stp x12, x13, [x0, #32]  
	stp x14, x15, [x0, #48]  
	ldp x12, x13, [x3, #64]  
	ldp x14, x15, [x3, #80]  
	stp x12, x13, [x0, #64]  
	stp x14, x15, [x0, #80]  
	ldp x12, x13, [x3, #96]  
	ldp x14, x15, [x3, #112]  
	stp x12, x13, [x0, #96]  
	stp x14, x15, [x0, #112]  
	ldr x1, [x3, #128] // load the overflow limb into x1, reduce_once wants it that way 
	bl _reduce_once 
	/* Restore Stack */
	ldp lr, x0, [sp, #0]
	ldp x19, x20, [sp, #16]
	ldp x21, x22, [sp, #32]
	ldp x23, x24, [sp, #48]
	ldp x25, x26, [sp, #64]
	ldp x27, x28, [sp, #80]
	ldr x30, [sp, #96]
	add sp, sp, #240
	ret 
/*
 [x0] = [x0]x1 - [p] mod [p] 
 x1 = potential overflow  
*/
.global _reduce_once
_reduce_once: 
	adrp x15, uintbig_p@PAGE
	add x15, x15, uintbig_p@PAGEOFF
	/* Limbs 0 - 3 */
	ldp x3, x4, [x0, #0]  // load A 
	ldp x5, x6, [x0, #16]  // load A 
	ldp x7, x8, [x15, #0]  // load P 
	ldp x9, x10, [x15, #16]  // load P 
	subs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x0, #0]  // store result 
	stp x13, x14, [x0, #16]  // store result 
	/* Limbs 4 - 7 */
	ldp x3, x4, [x0, #32]  // load A 
	ldp x5, x6, [x0, #48]  // load A 
	ldp x7, x8, [x15, #32]  // load P 
	ldp x9, x10, [x15, #48]  // load P 
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x0, #32]  // store result 
	stp x13, x14, [x0, #48]  // store result 
	/* Limbs 8 - 11 */
	ldp x3, x4, [x0, #64]  // load A 
	ldp x5, x6, [x0, #80]  // load A 
	ldp x7, x8, [x15, #64]  // load P 
	ldp x9, x10, [x15, #80]  // load P 
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x0, #64]  // store result 
	stp x13, x14, [x0, #80]  // store result 
	/* Limbs 12 - 15 */
	ldp x3, x4, [x0, #96]  // load A 
	ldp x5, x6, [x0, #112]  // load A 
	ldp x7, x8, [x15, #96]  // load P 
	ldp x9, x10, [x15, #112]  // load P 
	sbcs x11, x3, x7  
	sbcs x12, x4, x8  
	sbcs x13, x5, x9  
	sbcs x14, x6, x10  
	stp x11, x12, [x0, #96]  // store result 
	stp x13, x14, [x0, #112]  // store result 
	/* Final carry of a+b-p */
	sbcs x1, x1, xzr  // potential overflow of a+b 
	sbcs x1, xzr, xzr  // if a-p negative, carry is 1 
	/* AND P and a + p */
	/* Limbs 0 - 3 */
	ldp x3, x4, [x0, #0]  // load A 
	ldp x5, x6, [x0, #16]  // load A 
	ldp x7, x8, [x15, #0]  // load P 
	ldp x9, x10, [x15, #16]  // load P 
	and x7, x7, x1  
	and x8, x8, x1  
	and x9, x9, x1  
	and x10, x10, x1  

	adds x11, x3, x7  
	adcs x12, x4, x8  
	adcs x13, x5, x9  
	adcs x14, x6, x10  
	stp x11, x12, [x0, #0]  // store result 
	stp x13, x14, [x0, #16]  // store result 
	/* Limbs 4 - 7 */
	ldp x3, x4, [x0, #32]  // load A 
	ldp x5, x6, [x0, #48]  // load A 
	ldp x7, x8, [x15, #32]  // load P 
	ldp x9, x10, [x15, #48]  // load P 
	and x7, x7, x1  
	and x8, x8, x1  
	and x9, x9, x1  
	and x10, x10, x1  

	adcs x11, x3, x7  
	adcs x12, x4, x8  
	adcs x13, x5, x9  
	adcs x14, x6, x10  
	stp x11, x12, [x0, #32]  // store result 
	stp x13, x14, [x0, #48]  // store result 
	/* Limbs 8 - 11 */
	ldp x3, x4, [x0, #64]  // load A 
	ldp x5, x6, [x0, #80]  // load A 
	ldp x7, x8, [x15, #64]  // load P 
	ldp x9, x10, [x15, #80]  // load P 
	and x7, x7, x1  
	and x8, x8, x1  
	and x9, x9, x1  
	and x10, x10, x1  

	adcs x11, x3, x7  
	adcs x12, x4, x8  
	adcs x13, x5, x9  
	adcs x14, x6, x10  
	stp x11, x12, [x0, #64]  // store result 
	stp x13, x14, [x0, #80]  // store result 
	/* Limbs 12 - 15 */
	ldp x3, x4, [x0, #96]  // load A 
	ldp x5, x6, [x0, #112]  // load A 
	ldp x7, x8, [x15, #96]  // load P 
	ldp x9, x10, [x15, #112]  // load P 
	and x7, x7, x1  
	and x8, x8, x1  
	and x9, x9, x1  
	and x10, x10, x1  

	adcs x11, x3, x7  
	adcs x12, x4, x8  
	adcs x13, x5, x9  
	adcs x14, x6, x10  
	stp x11, x12, [x0, #96]  // store result 
	stp x13, x14, [x0, #112]  // store result 
	ret 


